{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ft_detection.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyPU8oo3wnrA9PMiAT7pOt5b"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ifDc-cTGkR6U","executionInfo":{"status":"ok","timestamp":1648922670166,"user_tz":-60,"elapsed":2596,"user":{"displayName":"Χρήστος Μανιάτης","userId":"00852017530683764808"}},"outputId":"8603a23a-453a-4893-9c1f-9eef3c4f6852"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["Feature detection with SCRaPL, Spearman and Pearson."],"metadata":{"id":"SeGxhtKtpKgf"}},{"cell_type":"code","metadata":{"id":"e6buIZRqkZT9"},"source":["from IPython import display\n","import pandas as pd\n","import numpy as np\n","import numpy.ma as ma\n","import scipy\n","import scipy.stats\n","from scipy.stats import gaussian_kde\n","from scipy.stats import t\n","\n","from matplotlib import pyplot as plt\n","from matplotlib import colors\n","from matplotlib.ticker import PercentFormatter\n","from matplotlib import cm\n","from matplotlib.colors import Normalize \n","from matplotlib.offsetbox import AnchoredText\n","\n","from sklearn.neighbors import KernelDensity\n","\n","import tensorflow_probability as tfp\n","import tensorflow.compat.v2 as tf\n","tf.enable_v2_behavior()\n","\n","import pickle\n","from timeit import default_timer as timer\n","Folder = '/content/drive/MyDrive/SCRaPL/'\n","Techn = '10X'\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Required Functions"],"metadata":{"id":"8btKTqYRa06k"}},{"cell_type":"code","source":["#Numerical integral estimation\n","def integral(y, x,axis=0):\n","    if axis == 0:\n","        dx = (x[-1,:] - x[0,:]) / (int(x.shape[0]) - 1)\n","        num_int = np.multiply((y[0,:] + y[-1,:])/2+np.sum(y[1:-1,:],axis=0) , dx)\n","    else: \n","        dx = (x[:,-1] - x[:,0]) / (int(x.shape[1]) - 1)\n","        num_int = np.multiply((y[:,0] + y[:,-1])/2+np.sum(y[:,1:-1],axis=1) , dx)\n","    return  num_int\n","\n","#SCRaPL Feature Detection\n","def ft_detect(alpha,prob_bay):\n","\n","      p_c = 1-prob_bay\n","      zz1 = tf.greater(p_c,alpha)\n","      zz2 = tf.less_equal(p_c,alpha)  \n","\n","      EFDR = tf.reduce_mean(tf.gather(prob_bay,tf.where(zz1==True)))\n","      num_fts = tf.shape(tf.where(zz1==True))[0]\n","      fts_ind = tf.cast(zz1,dtype=tf.int16)\n","\n","      return num_fts,EFDR,fts_ind\n","\n","#Multiple hypothesis testing correction.  (Used for Spearman and Pearson)\n","#Taken from https://github.com/CoBiG2/cobig_misc_scripts/blob/master/FDR.py\n","#Author: Francisco Pina Martins <f.pinamartins@gmail.com>\n","#Taken from https://stackoverflow.com/a/21739593/3091595\n","def multiple_testing_correction(pvalues, correction_type=\"FDR\"):\n","\n","    from numpy import array, empty\n","    pvalues = array(pvalues)\n","    sample_size = pvalues.shape[0]\n","    qvalues = empty(sample_size)\n","    if correction_type == \"Bonferroni\":\n","        # Bonferroni correction\n","        qvalues = sample_size * pvalues\n","    elif correction_type == \"Bonferroni-Holm\":\n","        # Bonferroni-Holm correction\n","        values = [(pvalue, i) for i, pvalue in enumerate(pvalues)]\n","        values.sort()\n","        for rank, vals in enumerate(values):\n","            pvalue, i = vals\n","            qvalues[i] = (sample_size-rank) * pvalue\n","    elif correction_type == \"FDR\":\n","        # Benjamini-Hochberg, AKA - FDR test\n","        values = [(pvalue, i) for i, pvalue in enumerate(pvalues)]\n","        values.sort()\n","        values.reverse()\n","        new_values = []\n","        for i, vals in enumerate(values):\n","            rank = sample_size - i\n","            pvalue, index = vals\n","            new_values.append((sample_size/rank) * pvalue)\n","        for i in range(0, int(sample_size)-1):\n","            if new_values[i] < new_values[i+1]:\n","                new_values[i+1] = new_values[i]\n","        for i, vals in enumerate(values):\n","            pvalue, index = vals\n","            qvalues[index] = new_values[i]\n","    return qvalues\n","\n","#Estimate null hypothesis distribution for Pearson/Spearman. (Supplementary Materials S9.1)\n","def null_dist(df,null_thrs):\n","      r = np.linspace(-0.999,0.999,num=499,endpoint=True)\n","      rho = np.linspace(-null_thrs-0.001,null_thrs+0.001,num=1000,endpoint=True)\n","      r,rho = np.meshgrid(r,rho)\n","      z_nrm = np.log(df-2) +np.math.lgamma(df-1)-0.5*np.log(2*np.math.pi)-np.math.lgamma(df-0.5)\n","      z = 0.5*(df-1)*np.log(1-np.square(rho))+0.5*(df-4)*np.log(1-np.square(r))-(df-1.5)*np.log(1-np.multiply(rho,r))\n","      f = np.exp(z+z_nrm)\n","      ff = scipy.special.hyp2f1(0.5,0.5,0.5*(2*df-1),0.5*(1+np.multiply(r,rho)))\n","      p = np.multiply(f,ff)\n","      p = integral(p[np.abs(rho[:,0])<=null_thrs,:],rho[np.abs(rho[:,0])<=null_thrs,:],0)\n","      p = p/integral(p[:,np.newaxis],r[0,:][:,np.newaxis],0)\n","      return p,r[0,:]\n","\n","#Pearson correlation ignoring readings with zeros methylation coverage\n","def nancorrcoef(x,y,num_obs):\n","\n","      tt1 = np.divide(x-np.nanmean(x,axis=1)[:,np.newaxis],np.nanstd(x,axis=1,ddof=1)[:,np.newaxis])\n","      tt2 = np.divide(y-np.nanmean(y,axis=1)[:,np.newaxis],np.nanstd(y,axis=1,ddof=1)[:,np.newaxis])\n","\n","      crr = np.divide(np.nansum(np.multiply(tt1,tt2),axis=1)[:,np.newaxis],num_obs-1)\n","      return crr"],"metadata":{"id":"OhaDD2cL45HC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load Data"],"metadata":{"id":"YRmkYGYP5Hyi"}},{"cell_type":"code","source":["if Techn == 'NMT':\n","\n","    with open(Folder+'Demo/Data/yy_met_300_100_'+Techn+'.pickle','rb') as handle: \n","        yy_met = pickle.load(handle)\n","    with open(Folder+'Demo/Data/yy_exp_300_100_'+Techn+'.pickle','rb') as handle: \n","        yy_exp = pickle.load(handle)\n","    with open(Folder+'Demo/Data/yy_cpg_300_100_'+Techn+'.pickle','rb') as handle: \n","        CpG = pickle.load(handle)\n","    with open(Folder+'Demo/Data/Norm_300_100_'+Techn+'.pickle','rb') as handle: \n","        nrm = pickle.load(handle)\n","    \n","    n_genes = yy_exp.shape[0]\n","    yy_met_nrm = tf.divide(yy_met,CpG).numpy()\n","    yy_exp_nrm = tf.divide(yy_exp,nrm).numpy()\n","    CpG_np = CpG.numpy()\n","    \n","    CpG_np[CpG_np ==0] = np.nan\n","    yy_met_nrm[np.isnan(CpG_np)] = np.nan\n","    yy_exp_nrm[np.isnan(CpG_np)] = np.nan \n","    num_obs = np.sum(~np.isnan(CpG_np),axis=1)[:,np.newaxis]\n","\n","elif Techn == '10X':\n","\n","    with open(Folder+'Demo/Data/yy_acc_300_100_'+Techn+'.pickle','rb') as handle: \n","        yy_acc = pickle.load(handle)\n","    with open(Folder+'Demo/Data/yy_exp_300_100_'+Techn+'.pickle','rb') as handle: \n","        yy_exp = pickle.load(handle)\n","    with open(Folder+'Demo/Data/Norm_acc_300_100_'+Techn+'.pickle','rb') as handle: \n","        nrm_acc = pickle.load(handle)\n","    with open(Folder+'Demo/Data/Norm_exp_300_100_'+Techn+'.pickle','rb') as handle: \n","        nrm_exp = pickle.load(handle)\n","\n","    n_genes = yy_exp.shape[0]\n","    yy_acc_nrm = tf.divide(yy_acc,nrm_acc).numpy()\n","    yy_exp_nrm = tf.divide(yy_exp,nrm_exp).numpy()\n","\n","else:\n","        \n","    print('Please choose a correct input')\n","\n","with open(Folder+'Demo/Results/nuts_cor_300_100_'+Techn+'.pickle', 'rb') as handle:\n","    cor = pickle.load(handle).numpy()\n"],"metadata":{"id":"WWqz9Ysm5HHc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["SCRaPL Freature Detection"],"metadata":{"id":"k3ecI5QhVP_-"}},{"cell_type":"code","source":["#Kernel Desnity Estimation for posterior correlation samples\n","x_support = np.linspace(-3, 3, 400)\n","kde_obj= np.apply_along_axis(lambda x: KernelDensity(bandwidth=0.1, kernel='gaussian').fit(x_support[:,None]),0, cor)\n","\n","#Estimate tail probability\n","p_bay  = np.zeros(n_genes)\n","gam = .115\n","for ii in range(n_genes):\n","    density =  np.exp(kde_obj[ii].score_samples(x_support[:,None]))\n","    p_bay[ii] =  integral(density[None,np.abs(x_support)<2*np.math.atanh(gam)], x_support[None,np.abs(x_support)<2*np.math.atanh(gam)],axis=1)\n","\n","#Grid search for optimal alpha value estimation and feature detection.\n","alpha = np.linspace(0.0, 1.0, 201)\n","for ii in range(len(alpha)):\n","    num_features,EFDR,ft_ind = ft_detect(alpha[ii],p_bay)\n","    if EFDR<0.1:\n","          summary = tf.stack([num_features.numpy(),EFDR.numpy(),alpha[ii]]).numpy()\n","          print(summary)\n","          break\n","    else:\n","          print(\"No significant feature detected\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jJk746pA_kMP","executionInfo":{"status":"ok","timestamp":1648922677320,"user_tz":-60,"elapsed":3181,"user":{"displayName":"Χρήστος Μανιάτης","userId":"00852017530683764808"}},"outputId":"0a83fbdf-350d-4d8c-c80f-d581f12269d5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[3.00e+02 7.25e-02 0.00e+00]\n"]}]},{"cell_type":"markdown","source":["Frequentist Feature Detection"],"metadata":{"id":"n3x2tny-YIf9"}},{"cell_type":"code","source":["t_prs = np.ones(n_genes)\n","t_spr = np.ones(n_genes)\n","\n","if Techn =='10X':\n","\n","      cor_prs = np.diag(np.corrcoef(yy_acc_nrm,yy_exp_nrm)[:n_genes,n_genes:] )\n","      cor_spr, _ = scipy.stats.spearmanr(yy_acc_nrm,yy_exp_nrm,axis=1)\n","      cor_spr =  np.diag(cor_spr[:n_genes,n_genes:])\n","\n","      p_a,r=null_dist(n_genes-2,gam)\n","      for ii in range(n_genes): \n","\n","            \n","            thrs_spr=-abs(cor_spr[ii])\n","            thrs_prs=-abs(cor_prs[ii])\n","\n","            sp_spr = r[r<=thrs_spr]\n","            sp_prs = r[r<=thrs_prs]\n","\n","            t_spr[ii] = 2*integral(p_a[r<=thrs_spr][:,np.newaxis],sp_spr[:,np.newaxis],0)\n","            t_prs[ii] = 2*integral(p_a[r<=thrs_prs][:,np.newaxis],sp_prs[:,np.newaxis],0)\n","\n","      p_adj_prs = multiple_testing_correction(t_prs, correction_type = \"FDR\")\n","      p_adj_spr = multiple_testing_correction(t_spr, correction_type = \"FDR\")\n","\n","      print(\"Prs_det_ft:{},Spr_det_ft:{}\".format(np.sum(p_adj_prs<0.1),np.sum(p_adj_spr<0.1)))\n","\n","else:\n","\n","      cor_prs = nancorrcoef(yy_met_nrm,yy_exp_nrm,num_obs)\n","      cor_spr = np.zeros(n_genes)\n","      for ii in range(n_genes):\n","          cor_spr[ii] , _ = scipy.stats.spearmanr(yy_met_nrm[ii,:],yy_exp_nrm[ii,:],nan_policy='omit')\n","\n","     \n","      xx = np.apply_along_axis(lambda x: null_dist(x-2,gam),1, num_obs)\n","\n","\n","      for ii in range(n_genes): \n","\n","          p_a = np.squeeze(xx[ii,0,:])\n","          r = np.squeeze(xx[ii,1,:])\n","          thrs_spr=-abs(cor_spr[ii])\n","          thrs_prs=-abs(cor_prs[ii])\n","\n","          sp_spr = r[r<=thrs_spr]\n","          sp_prs = r[r<=thrs_prs]\n","\n","          t_spr[ii] = 2*integral(p_a[r<=thrs_spr][:,np.newaxis],sp_spr[:,np.newaxis],0)\n","          t_prs[ii] = 2*integral(p_a[r<=thrs_prs][:,np.newaxis],sp_prs[:,np.newaxis],0)\n","\n","      p_adj_prs = multiple_testing_correction(t_prs, correction_type = \"FDR\")\n","      p_adj_spr = multiple_testing_correction(t_spr, correction_type = \"FDR\")\n","\n","      print(\"Prs_det_ft:{},Spr_det_ft:{}\".format(np.sum(p_adj_prs<0.1),np.sum(p_adj_spr<0.1)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cSauCsVSohaC","executionInfo":{"status":"ok","timestamp":1648922677847,"user_tz":-60,"elapsed":537,"user":{"displayName":"Χρήστος Μανιάτης","userId":"00852017530683764808"}},"outputId":"4ffa9831-6f12-4e49-fbfc-6a6aa247be35"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Prs_det_ft:23,Spr_det_ft:24\n"]}]}]}