{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ft_detection_brain_Pearson.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyMC0dwei60EgqDf1Mxpp01K"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ifDc-cTGkR6U","executionInfo":{"status":"ok","timestamp":1635672080616,"user_tz":0,"elapsed":27928,"user":{"displayName":"Χρήστος Μανιάτης","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00852017530683764808"}},"outputId":"58ef8ecc-89ec-4fe1-ae6d-7bcf16bad027"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"Up2DG0qC2CrE"},"source":["This script is used to estimate the number of features with strong regulatory action using Pearson correlation in brain data."]},{"cell_type":"code","metadata":{"id":"e6buIZRqkZT9"},"source":["from IPython import display\n","import pandas as pd\n","import numpy as np\n","import numpy as np\n","import numpy.ma as ma\n","import scipy\n","import scipy.stats\n","from scipy.stats import gaussian_kde\n","from scipy.stats import t\n","\n","from matplotlib import pyplot as plt\n","from matplotlib import colors\n","from matplotlib.ticker import PercentFormatter\n","from matplotlib import cm\n","from matplotlib.colors import Normalize \n","from matplotlib.offsetbox import AnchoredText\n","\n","from tensorflow import keras\n","from sklearn.neighbors import KernelDensity\n","\n","from tensorflow.keras import layers\n","import tensorflow_probability as tfp\n","import tensorflow.compat.v2 as tf\n","tf.enable_v2_behavior()\n","\n","import pickle\n","from timeit import default_timer as timer\n","Folder = '/content/drive/MyDrive/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QggXX8xzd8Qk"},"source":["Load data, estimate Pearson correlation and construct null distribution hypothesis."]},{"cell_type":"code","metadata":{"id":"G-ckpGt7Aok6"},"source":["yy_acc_pd = pd.read_csv(Folder+'SCRaPL/Real/Data/Acc_atac.csv',',',header=[0],index_col=[0])\n","yy_exp_pd = pd.read_csv(Folder+'SCRaPL/Real/Data/Rna_atac.csv',',',header=[0],index_col=[0])\n","Norm = pd.read_csv(Folder+'SCRaPL/Real/Data/Nrm_atac.csv',',',index_col=[1])\n","Norm = Norm.drop(columns=Norm.columns[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"crkiGkE1AtfC"},"source":["tt1 = yy_acc_pd.div(Norm['nrm_acc']).to_numpy()\n","tt2 = yy_exp_pd.div(Norm['nrm_rna']).to_numpy()\n","corr = np.corrcoef(tt1,tt2)\n","crr_prs = np.diag(corr[:4249,4249:])\n","num_obs = tt1.shape[1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a4G5YNRgA1UF"},"source":["#Taken from https://github.com/CoBiG2/cobig_misc_scripts/blob/master/FDR.py\n","#Author: Francisco Pina Martins <f.pinamartins@gmail.com>\n","#Taken from https://stackoverflow.com/a/21739593/3091595\n","\n","def multiple_testing_correction(pvalues, correction_type=\"FDR\"):\n","\n","    from numpy import array, empty\n","    pvalues = array(pvalues)\n","    sample_size = pvalues.shape[0]\n","    qvalues = empty(sample_size)\n","    if correction_type == \"Bonferroni\":\n","        # Bonferroni correction\n","        qvalues = sample_size * pvalues\n","    elif correction_type == \"Bonferroni-Holm\":\n","        # Bonferroni-Holm correction\n","        values = [(pvalue, i) for i, pvalue in enumerate(pvalues)]\n","        values.sort()\n","        for rank, vals in enumerate(values):\n","            pvalue, i = vals\n","            qvalues[i] = (sample_size-rank) * pvalue\n","    elif correction_type == \"FDR\":\n","        # Benjamini-Hochberg, AKA - FDR test\n","        values = [(pvalue, i) for i, pvalue in enumerate(pvalues)]\n","        values.sort()\n","        values.reverse()\n","        new_values = []\n","        for i, vals in enumerate(values):\n","            rank = sample_size - i\n","            pvalue, index = vals\n","            new_values.append((sample_size/rank) * pvalue)\n","        for i in range(0, int(sample_size)-1):\n","            if new_values[i] < new_values[i+1]:\n","                new_values[i+1] = new_values[i]\n","        for i, vals in enumerate(values):\n","            pvalue, index = vals\n","            qvalues[index] = new_values[i]\n","    return qvalues\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"akXU0bQQYn0h"},"source":["def integral(y, x,axis=0):\n","    if axis == 0:\n","        dx = (x[-1,:] - x[0,:]) / (int(x.shape[0]) - 1)\n","        num_int = np.multiply((y[0,:] + y[-1,:])/2+np.sum(y[1:-1,:],axis=0) , dx)\n","    else: \n","        dx = (x[:,-1] - x[:,0]) / (int(x.shape[1]) - 1)\n","        num_int = np.multiply((y[:,0] + y[:,-1])/2+np.sum(y[:,1:-1],axis=1) , dx)\n","    return  num_int\n","\n","#Estimate null hypothesis distribution for Pearson.\n","def null_dist(df,null_thrs):\n","      r = np.linspace(-0.999,0.999,num=1999,endpoint=True)\n","      rho = np.linspace(-null_thrs-0.001,null_thrs+0.001,num=1000,endpoint=True)\n","      r,rho = np.meshgrid(r,rho)\n","      z_nrm = np.log(df-2) +np.math.lgamma(df-1)-0.5*np.log(2*np.math.pi)-np.math.lgamma(df-0.5)\n","      z = 0.5*(df-1)*np.log(1-np.square(rho))+0.5*(df-4)*np.log(1-np.square(r))-(df-1.5)*np.log(1-np.multiply(rho,r))\n","      f = np.exp(z+z_nrm)\n","      ff = scipy.special.hyp2f1(0.5,0.5,0.5*(2*df-1),0.5*(1+np.multiply(r,rho)))\n","      p = np.multiply(f,ff)\n","      p = integral(p[np.abs(rho[:,0])<=null_thrs,:],rho[np.abs(rho[:,0])<=null_thrs,:],0)\n","      p = p/integral(p[:,np.newaxis],r[0,:][:,np.newaxis],0)\n","      return p,r[0,:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6SN5sqIF0T38"},"source":["t= np.ones(4249)\n","null_thrs = 0.145\n","\n","for ii in range(t.shape[0]): \n","    p_a,r=null_dist(num_obs,null_thrs)\n","    thrs=-abs(crr_prs[ii])\n","    sp = r[r<=thrs]\n","    t[ii] = 2*integral(p_a[r<=thrs][:,np.newaxis],sp[:,np.newaxis],0)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HJRj1e-_A3wp"},"source":["p_adj = multiple_testing_correction(t, correction_type = \"FDR\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"86BIVTXPSWZQ"},"source":["np.savetxt(Folder+'SCRaPL/Real/Paper_Analysis/Pearson_meta/brain_p_value.csv', t, delimiter=\",\")\n","np.savetxt(Folder+'SCRaPL/Real/Paper_Analysis/Pearson_meta/brain_p_value_adj.csv', p_adj, delimiter=\",\")"],"execution_count":null,"outputs":[]}]}