{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SCRaPL_gastr_no_inf_dic.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyMp8Dap61PKa8PTKQ7oWjg7"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ifDc-cTGkR6U","executionInfo":{"status":"ok","timestamp":1632506988364,"user_tz":-60,"elapsed":465,"user":{"displayName":"Χρήστος Μανιάτης","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00852017530683764808"}},"outputId":"53c69932-6b61-4f61-e0dd-2839f33c739a"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["This script is used to  get samples which are later used to estimate DIC of SCRaPL without inflation on mESC data.The only difference from standard inference scripts is that it stores some extra parameters like log likelihood tied to a particular sample and average of posterior latent methylation and expression."],"metadata":{"id":"l_kvnXKLNb6c"}},{"cell_type":"code","metadata":{"id":"e6buIZRqkZT9"},"source":["from IPython import display\n","import pandas as pd\n","import numpy as np\n","import scipy\n","import scipy.stats\n","\n","from matplotlib import pyplot as plt\n","from matplotlib import colors\n","from matplotlib.ticker import PercentFormatter\n","from tensorflow import keras\n","\n","from tensorflow.keras import layers\n","import tensorflow_probability as tfp\n","import tensorflow.compat.v2 as tf\n","tf.enable_v2_behavior()\n","\n","import pickle\n","from timeit import default_timer as timer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3bO5QWdjkbqt"},"source":["tfd = tfp.distributions\n","tfb = tfp.bijectors\n","Folder = '/content/drive/MyDrive/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vbpaJa0Dk7qA"},"source":["#Load methylation, expression data and cell normalization constants\n","yy_met = pd.read_csv(Folder+'SCRaPL/Real/Data/Met.csv',sep=',',index_col=[0])\n","yy_exp = pd.read_csv(Folder+'SCRaPL/Real/Data/Rna.csv',sep=',',index_col=[0])\n","CpG = pd.read_csv(Folder+'SCRaPL/Real/Data/CpG.csv',sep=',',index_col=[0])\n","nrm = pd.read_csv(Folder+'SCRaPL/Real/Data/nrm.csv',sep=',',index_col=[1])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5wf0uC_Qoik8"},"source":["Norm = tf.transpose(tf.convert_to_tensor(nrm,dtype=tf.float32))\n","yy_met = tf.convert_to_tensor(yy_met,dtype=tf.float32)\n","CpG = tf.convert_to_tensor(CpG,dtype=tf.float32)\n","yy_exp = tf.convert_to_tensor(yy_exp,dtype=tf.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L3U5p_L1vzN4"},"source":["x_genes,x_cells = tf.shape(CpG)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tF14OYMokc9U"},"source":["aff = tfb.Chain([tfb.Shift(-1.),tfb.Scale(scale=2.)])\n","aff_inv = tfb.Invert(aff)\n","\n","exp = tfb.Exp()\n","log = tfb.Invert(exp)\n","\n","tanh = tfb.Tanh()\n","tanh_inv = tfb.Invert(tanh)\n","\n","sigm = tfb.Sigmoid()\n","sigm_inv = tfb.Invert(sigm)\n","\n","cor_trsf = tfb.Chain([aff_inv,tanh,tfb.Scale(scale=0.5)])\n","cor_trsf_inv = tfb.Invert(cor_trsf)\n","\n","eps=0.001\n","bin_bij = tfb.Chain([tfb.Shift(eps/2.0),tfb.Scale(scale=1.0-eps),tfb.NormalCDF()])\n","\n","cor_bij = tfb.Chain([tanh,tfb.Scale(scale=0.5)])\n","std_bij = tfb.Chain([exp,tfb.Scale(scale=-1.0)])\n","sqr_bij = tfb.Square()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-B87ra33lOe8"},"source":["#SCRaPL's graphical model\n","def SCRaPL(N_genes,N_cells,Cover,Nrm):\n","    def prior():\n","        cor_lt = yield tfd.TransformedDistribution( distribution = tfd.Beta( concentration0 = 15.0*tf.ones([N_genes,1]), concentration1=15.0*tf.ones([N_genes,1])), bijector= cor_trsf_inv, name = \"cor_lt\" )\n","        m_met_lt = yield tfd.Normal(loc=0*tf.ones([N_genes,1]),scale=tf.ones([N_genes,1]), name = \"m_met_lt\")\n","        m_exp_lt = yield tfd.Normal(loc=4*tf.ones([N_genes,1]),scale=tf.ones([N_genes,1]), name = \"m_exp_lt\")\n","        s_met_lt = yield tfd.TransformedDistribution( distribution = tfd.InverseGamma(concentration=2.5*tf.ones([N_genes,1]),scale=4.5*tf.ones([N_genes,1])),bijector= log, name = \"s_met_lt\" )\n","        s_exp_lt = yield tfd.TransformedDistribution( distribution = tfd.InverseGamma(concentration=2.5*tf.ones([N_genes,1]),scale=4.5*tf.ones([N_genes,1])),bijector= log , name = \"s_exp_lt\")\n","\n","        cor = cor_bij.forward(cor_lt)\n","        s_met = std_bij.forward(s_met_lt)\n","        s_exp = std_bij.forward(s_exp_lt)\n","        \n","        mm_met = tf.math.multiply( m_met_lt,tf.ones([N_genes,N_cells]))\n","        mm_exp = tf.math.multiply( m_exp_lt,tf.ones([N_genes,N_cells]))\n","        ss_met = tf.math.multiply( s_met   ,tf.ones([N_genes,N_cells]))\n","        ss_exp = tf.math.multiply( s_exp   ,tf.ones([N_genes,N_cells]))\n","        ccor =   tf.math.multiply( cor     ,tf.ones([N_genes,N_cells])) \n","        nrm = tf.math.multiply( log.forward(Nrm),tf.ones([N_genes,1]))\n","\n","        x_met = yield tfd.Normal(loc = mm_met, scale = ss_met,name=\"x_met\")\n","        m_cnd_exp = mm_exp+tf.math.multiply(tf.math.divide(tf.math.multiply(ss_exp,x_met-mm_met),ss_met),ccor)\n","        s_cnd_exp = tf.math.sqrt(tf.math.multiply(1-tf.math.square(ccor),tf.math.square(ss_exp)))\n","\n","        x_exp = yield tfd.Normal(loc = m_cnd_exp, scale = s_cnd_exp,name=\"x_exp\")\n","\n","        rt_bin = bin_bij.forward(x_met)\n","\n","        y_met = yield tfd.Binomial(total_count=Cover,probs=rt_bin,name=\"y_met\")\n","        y_exp = yield tfd.Poisson(log_rate=x_exp+nrm,name=\"y_exp\")\n","\n","    comp_var_coroutine = tfd.JointDistributionCoroutineAutoBatched(prior)\n","    return comp_var_coroutine\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RenvbD10zSxi"},"source":["#SCRaPL without priors used in DIC estimation.\n","def SCRaPL_dic(N_genes,N_cells,Cover,Nrm,param):\n","    cor_lt,m_met_lt,m_exp_lt,s_met_lt,s_exp_lt = param\n","    def prior():\n","        Root = tfd.JointDistributionCoroutine.Root\n","        cor = cor_bij.forward(cor_lt)\n","        s_met = std_bij.forward(s_met_lt)\n","        s_exp = std_bij.forward(s_exp_lt)\n","        \n","        mm_met = tf.math.multiply( m_met_lt,tf.ones([1,N_genes,N_cells]))\n","        mm_exp = tf.math.multiply( m_exp_lt,tf.ones([1,N_genes,N_cells]))\n","        ss_met = tf.math.multiply( s_met   ,tf.ones([1,N_genes,N_cells]))\n","        ss_exp = tf.math.multiply( s_exp   ,tf.ones([1,N_genes,N_cells]))\n","        ccor =   tf.math.multiply( cor     ,tf.ones([1,N_genes,N_cells]))\n","        nrm = tf.math.multiply( log.forward(Nrm),tf.ones([1,N_genes,1]))\n","\n","        x_met = yield Root(tfd.Independent(tfd.Normal(loc = mm_met, scale = ss_met,name=\"x_met\"),reinterpreted_batch_ndims=1))\n","        m_cnd_exp = mm_exp+tf.math.multiply(tf.math.divide(tf.math.multiply(ss_exp,x_met-mm_met),ss_met),ccor)\n","        s_cnd_exp = tf.math.sqrt(tf.math.multiply(1-tf.math.square(ccor),tf.math.square(ss_exp)))\n","\n","        x_exp = yield Root(tfd.Independent(tfd.Normal(loc = m_cnd_exp, scale = s_cnd_exp,name=\"x_exp\"),reinterpreted_batch_ndims=1))\n","\n","        rt_bin = bin_bij.forward(x_met)\n","\n","        y_met = yield Root(tfd.Independent(tfd.Binomial(total_count=Cover,probs=rt_bin,name=\"y_met\"),reinterpreted_batch_ndims=1))\n","        y_exp = yield Root(tfd.Independent(tfd.Poisson(log_rate=x_exp+nrm, name=\"y_exp\") , reinterpreted_batch_ndims=1)   )\n","\n","    comp_var_coroutine = tfd.JointDistributionCoroutine(prior)\n","    return comp_var_coroutine"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PMRc41S-cm7c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632509805414,"user_tz":-60,"elapsed":2805262,"user":{"displayName":"Χρήστος Μανιάτης","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00852017530683764808"}},"outputId":"0d8fe174-bbdd-4f72-ee2a-7b87570a349a"},"source":["#As the dataset is too large to fit in memory we utilize genomic region independence assumption to perform sample chunks of 30 in each turn.\n","#As this step tends to be computationally demanding we halve the number of genomic regions to avoid the risk of running out of memory. Depending computational resources\n","#chunk size (www) has to be tuned \n","\n","prt = 1\n","time = []\n","www = 30\n","prts_tot = tf.math.ceil(x_genes/www)\n","jj = 0\n","\n","while prt < prts_tot+1:\n","          aa =  (prt-1)*www\n","          aa1 = prt*www\n","          yy_met_prt = yy_met[aa:aa1,:]\n","          yy_exp_prt = yy_exp[aa:aa1,:]\n","          CpG_prt = CpG[aa:aa1,:]\n","\n","          batch_num = tf.shape(CpG_prt)[0]\n","\n","          mdl_tr = SCRaPL(batch_num,x_cells,CpG_prt,Norm)\n","          init = mdl_tr.sample()\n","          vrr = [init[0],init[1],init[2],init[3],init[4],init[5],init[6]]\n","\n","          unconstrained_bijectors = [tfb.Identity(),tfb.Identity(),tfb.Identity(),tfb.Identity(),tfb.Identity(),tfb.Identity(),tfb.Identity()]\n","          @tf.function(autograph=False, jit_compile=True) \n","\n","          def sample_nuts():\n","\n","            init_x = vrr\n","            num_burnin_iter = 3000\n","            num_warmup_iter = int(0.8*num_burnin_iter)\n","            num_chain_iter = 2000\n","\n","            target_accept_rate = 0.65 \n","\n","            log_post = lambda x0,x1,x2,x3,x4,x5,x6: mdl_tr.log_prob(x0,x1,x2,x3,x4,x5,x6,yy_met_prt,yy_exp_prt)\n","\n","            def trace_fn(_, pkr):\n","                  return (\n","                      pkr.inner_results.inner_results.target_log_prob,\n","                      pkr.inner_results.inner_results.leapfrogs_taken,\n","                      pkr.inner_results.inner_results.has_divergence,\n","                      pkr.inner_results.inner_results.energy,\n","                      pkr.inner_results.inner_results.log_accept_ratio,\n","                      pkr.inner_results.inner_results.step_size\n","                        )\n","            nuts= tfp.mcmc.NoUTurnSampler(\n","                                      target_log_prob_fn=log_post,\n","                                      step_size=0.05,\n","                                      max_tree_depth=6\n","                                          ) \n","            ttk = tfp.mcmc.TransformedTransitionKernel(\n","                                      inner_kernel=nuts,\n","                                      bijector=unconstrained_bijectors\n","                                                          )\n","            adapted_kernel=tfp.mcmc.DualAveragingStepSizeAdaptation(\n","                                      inner_kernel=ttk,\n","                                      num_adaptation_steps=num_warmup_iter,\n","                                      target_accept_prob= target_accept_rate)\n","            \n","            states , sampler_stat =tfp.mcmc.sample_chain(\n","                              num_results=num_chain_iter,\n","                              num_burnin_steps=num_burnin_iter,\n","                              current_state=init_x,\n","                              kernel=adapted_kernel,\n","                              trace_fn=trace_fn) \n","\n","            return states, sampler_stat\n","\n","          start = timer()\n","          samples, sampler_stat = sample_nuts() \n","          end = timer()\n","          ttime = end-start\n","\n","          p_accept = tf.math.exp(tfp.math.reduce_logmeanexp(tf.minimum(sampler_stat[4], 0.)))\n","          stp_sz = sampler_stat[5][0][-1]\n","\n","          hmc_cor,hmc_m_met,hmc_m_exp,hmc_s_met,hmc_s_exp,hmc_x_met,hmc_x_exp = samples\n","\n","          crr_nuts = tf.squeeze(hmc_cor)\n","          m_met_nuts = tf.squeeze(hmc_m_met)\n","          m_exp_nuts = tf.squeeze(hmc_m_exp)\n","          s_met_nuts = tf.squeeze(hmc_s_met)\n","          s_exp_nuts = tf.squeeze(hmc_s_exp)\n","\n","\n","          mdl_dic = SCRaPL_dic(batch_num,x_cells,CpG_prt,Norm,[hmc_cor,hmc_m_met,hmc_m_exp,hmc_s_met,hmc_s_exp])\n","          llk_gastr = mdl_dic.log_prob(hmc_x_met,hmc_x_exp,yy_met_prt,yy_exp_prt)\n","\n","          x_met_mn = tf.reduce_mean(hmc_x_met,axis=0)\n","          x_exp_mn = tf.reduce_mean(hmc_x_exp,axis=0)\n","\n","          qc_acc_rt = tf.math.logical_and(p_accept<0.9,p_accept>0.4)\n","          qc_stp_sz = stp_sz>0.00001\n","\n","          if tf.math.logical_and(qc_acc_rt,qc_stp_sz)== True:\n","                time.append(ttime)\n","                with open(Folder+'SCRaPL/Real/Results_DIC/nuts_cor_gastr_ninf_'+str(prt)+'.pickle', 'wb') as handle:\n","                    pickle.dump(crr_nuts, handle)\n","                with open(Folder+'SCRaPL/Real/Results_DIC/nuts_m_met_gastr_ninf_'+str(prt)+'.pickle', 'wb') as handle:\n","                    pickle.dump(m_met_nuts, handle)\n","                with open(Folder+'SCRaPL/Real/Results_DIC/nuts_m_exp_gastr_ninf_'+str(prt)+'.pickle', 'wb') as handle:\n","                    pickle.dump(m_exp_nuts, handle)\n","\n","                with open(Folder+'SCRaPL/Real/Results_DIC/nuts_s_met_gastr_ninf_'+str(prt)+'.pickle', 'wb') as handle:\n","                    pickle.dump(s_met_nuts, handle)\n","                with open(Folder+'SCRaPL/Real/Results_DIC/nuts_s_exp_gastr_ninf_'+str(prt)+'.pickle', 'wb') as handle:\n","                    pickle.dump(s_exp_nuts, handle)\n","\n","                with open(Folder+'SCRaPL/Real/Results_DIC/log_prob_gastr_ninf_'+str(prt)+'.pickle', 'wb') as handle:\n","                    pickle.dump(llk_gastr, handle)\n","                with open(Folder+'SCRaPL/Real/Results_DIC/avg_exp_ninf_gastr_'+str(prt)+'.pickle', 'wb') as handle:\n","                    pickle.dump(x_exp_mn, handle)\n","                with open(Folder+'SCRaPL/Real/Results_DIC/avg_met_ninf_gastr_'+str(prt)+'.pickle', 'wb') as handle:\n","                    pickle.dump(x_met_mn, handle)     \n","                \n","                avg_time = tf.reduce_mean(tf.stack(time))\n","                rem_time = (tf.cast(prts_tot,dtype=tf.float32)-tf.cast(prt,dtype=tf.float32)) *avg_time\n","                print(tf.squeeze([rem_time,prt,stp_sz,p_accept]),tf.math.logical_and(qc_acc_rt,qc_stp_sz))\n","                prt+=1\n","                jj=0.0\n","          else:\n","                time.append(ttime)\n","                \n","                if jj>3.0:\n","                    skp_ind.append(prt)\n","                    prt+=1\n","                    jj=0.0\n","\n","                avg_time = tf.reduce_mean(tf.stack(time))\n","                rem_time = (tf.cast(prts_tot,dtype=tf.float32)-tf.cast(prt,dtype=tf.float32)) *avg_time\n","                print(tf.squeeze([rem_time,prt,stp_sz,p_accept]),tf.math.logical_and(qc_acc_rt,qc_stp_sz))\n","                jj+=1"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([2.5467537e+03 3.0100000e+02 1.0642152e-02 6.5197062e-01], shape=(4,), dtype=float32) tf.Tensor(True, shape=(), dtype=bool)\n","tf.Tensor([2.3759587e+03 3.0200000e+02 9.7204298e-03 6.4480865e-01], shape=(4,), dtype=float32) tf.Tensor(True, shape=(), dtype=bool)\n","tf.Tensor([2.2052170e+03 3.0300000e+02 9.2363283e-03 6.5441215e-01], shape=(4,), dtype=float32) tf.Tensor(True, shape=(), dtype=bool)\n","tf.Tensor([2.0393785e+03 3.0400000e+02 9.1340207e-03 6.5191185e-01], shape=(4,), dtype=float32) tf.Tensor(True, shape=(), dtype=bool)\n","WARNING:tensorflow:5 out of the last 5 calls to <function sample_nuts at 0x7fcca42ecb00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","tf.Tensor([1.8678354e+03 3.0500000e+02 8.7258648e-03 7.3664391e-01], shape=(4,), dtype=float32) tf.Tensor(True, shape=(), dtype=bool)\n","WARNING:tensorflow:6 out of the last 6 calls to <function sample_nuts at 0x7fcd22983200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","tf.Tensor([1.6987128e+03 3.0600000e+02 1.4317469e-02 6.7490411e-01], shape=(4,), dtype=float32) tf.Tensor(True, shape=(), dtype=bool)\n","tf.Tensor([1.5276816e+03 3.0700000e+02 1.1304198e-02 6.2265599e-01], shape=(4,), dtype=float32) tf.Tensor(True, shape=(), dtype=bool)\n","tf.Tensor([1.3569080e+03 3.0800000e+02 1.1461093e-02 7.1654934e-01], shape=(4,), dtype=float32) tf.Tensor(True, shape=(), dtype=bool)\n","tf.Tensor([1.1868491e+03 3.0900000e+02 1.1485932e-02 6.5941811e-01], shape=(4,), dtype=float32) tf.Tensor(True, shape=(), dtype=bool)\n","tf.Tensor([1.0174601e+03 3.1000000e+02 7.2580823e-03 6.8055171e-01], shape=(4,), dtype=float32) tf.Tensor(True, shape=(), dtype=bool)\n","tf.Tensor([8.4719781e+02 3.1100000e+02 1.1052814e-02 6.3168836e-01], shape=(4,), dtype=float32) tf.Tensor(True, shape=(), dtype=bool)\n","tf.Tensor([6.7745209e+02 3.1200000e+02 1.0117819e-02 6.9078022e-01], shape=(4,), dtype=float32) tf.Tensor(True, shape=(), dtype=bool)\n","tf.Tensor([5.0784000e+02 3.1300000e+02 1.4269154e-02 6.3571656e-01], shape=(4,), dtype=float32) tf.Tensor(True, shape=(), dtype=bool)\n","tf.Tensor([3.3863776e+02 3.1400000e+02 8.4929941e-03 6.5566498e-01], shape=(4,), dtype=float32) tf.Tensor(True, shape=(), dtype=bool)\n","tf.Tensor([1.6917836e+02 3.1500000e+02 9.6011069e-03 5.4957008e-01], shape=(4,), dtype=float32) tf.Tensor(True, shape=(), dtype=bool)\n","tf.Tensor([0.0000000e+00 3.1600000e+02 1.1563201e-02 5.7947218e-01], shape=(4,), dtype=float32) tf.Tensor(True, shape=(), dtype=bool)\n"]}]}]}